{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "EEP153 Grading Procedures\n=========================\n\n**Author:** Ethan Ligon\n\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## EEP153 Grading Procedures\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sources of Assessment\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Your grade in EEP153 will be based on the following different sources:\n\n1.  Others&rsquo; assessments of your teams&rsquo; performance on each class project;\n\n2.  Your teammates&rsquo; assessments of your contribution to each project;\n\n3.  The accuracy of your own *predictions* of others&rsquo; assessments; and\n\n4.  Some extra credit opportunities, including contributions to discussions.\n\n5.  An entirely optional final exam.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Data from Assessments\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Students\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Consider the following artificial data on assessment.  We first\ngenerate some data on (imaginary) students.  Each student is\ncharacterized by three things:\n\n-   Name (to identify student; we observe this);\n-   Ability (affects performance independent of effort);\n-   Effort (affects performance independent of ability);\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Names\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is way of getting a list of proper nouns, using shell tools.  You probably don&rsquo;t need to run this (we&rsquo;ve already provided a file of proper nouns).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "!wget -O - \"https://svnweb.freebsd.org/csrg/share/dict/words?view=co&content-type=text/plain\" | grep -o '\\<[A-Z][a-z]*\\>'  > proper_nouns"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The following code defines a function we can use to generate some random names for our imaginary students.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "#!pip install cufflinks\nimport cufflinks as cf\nimport pandas as pd\nimport numpy as np\nimport random\n\ndef random_names(n,k=2):\n    \"\"\"Return a list of n random k-part names.\n\n    Borrows from =amoodie='s amusing idea described at\n    https://stackoverflow.com/questions/18834636/random-word-generator-python\n    \"\"\"\n\n    name_words = open(\"proper_nouns\",'r').read().splitlines()\n\n    rand_name   = ' '.join([name_words[random.randint(0, len(name_words))]\n                            for i in range(k)])\n\n    names = []\n    for i in range(n):\n        names.append(' '.join([name_words[random.randint(0,len(name_words))]\n                               for j in range(k)]))\n\n    return names\n\nSTUDENTS = 40\nnames = random_names(STUDENTS)\n# print(names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Abilities & Effort\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Next, for each student we&rsquo;ll randomly draw an ability and an effort.\n Draws are from a normal distribution.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "ability = [random.normalvariate(0,1) for name in names]\n\neffort = [random.normalvariate(0,1) for name in names]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "With names, ability, and effort all determined, build a `pandas.DataFrame`.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "students = pd.DataFrame({'Ability':ability,'Effort':effort}, index=names)\n\nprint(students.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Performance\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "If professors could simply observe ability and effort, grading would\n be very easy!  That&rsquo;s not the world we live in, though.  Instead we\n have students take tests or complete assignments, where performance is\n related to effort and ability, and we try to draw inferences about\n the latter from the former.\n\nThe following code assigns students to random teams, and generates\nscores for their projects.  Note that, e.g., &ldquo;Team1&rdquo; means the\nassignment to teams for project 1; it&rsquo;s not an identifier for a team.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n# Assign students to random groups and generate project scores.\n\nPROJECTS = 4\nTEAMS = 8\n\nfor project in range(PROJECTS):\n    # Sort students into a random order\n    np.random.shuffle(names)\n\n    students = students.join(pd.Series(np.array([[i]*(STUDENTS // TEAMS)\n                                                 for i in range(TEAMS)]).flatten(),\n                                       index=names,name='Team%d' % (project+1,)))\nprint(students.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, performance on each group project is assumed to depend on the\n average of the of ability and effort for the entire team.  Every\n student will provide a *ranking* of all *other* teams&rsquo; projects.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "for project in range(1,PROJECTS+1):\n    teams = students.groupby('Team%d' % project)\n    teamscore = teams[['Ability','Effort']].mean().sum(axis=1) # Team averages\n    others_evals = teamscore.values.reshape((-1,1)) + np.random.randn(TEAMS,int(STUDENTS*(TEAMS-1)/TEAMS)) # Others' evals\n    others_evals = pd.DataFrame(others_evals).rank(ascending=False).mean(axis=1).squeeze() # Average of rankings\n    others_evals.name = 'Project%d' % project\n    students = students.join(others_evals,on='Team%d' % project)\n\nprint(students.filter(regex=\"Project\").head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So far so good; we have averages of rankings of all students for\n others&rsquo; team projects.  The second source of assessment are peer\n rankings *within* the group.  We assume that one&rsquo;s teammates provide\n rankings which depend on ability and effort, observed with error.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "for project in range(1,PROJECTS+1):\n    \n    peer_evals = students[['Ability','Effort']].sum(axis=1).values.reshape((-1,1))\n    peer_evals = peer_evals + np.random.randn(STUDENTS,STUDENTS//TEAMS + 1) # Error in obs.\n    peer_evals = pd.DataFrame(peer_evals,index=students.index).rank(ascending=False).mean(axis=1).squeeze() # Average of rankings\n    students['Peers%d' % project] = peer_evals\n\nprint(students.filter(regex=\"Peers\").head(10))\n#print(peer_evals)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Finally, there&rsquo;s also individual assessments, from the final exam and\n instructor assessment of discussion contributions.  These are also\n related to ability and effort, measured with error.  However, we\n assume that disucssion contributions depend more on effort than on\n ability.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "students['Final'] = students[['Ability','Effort']].sum(axis=1) \nstudents['Final'] = students['Final'] + np.random.randn(STUDENTS)*0.2\n\n# Effort weighted 0.7, ability 0.3\nstudents['Discussion'] = students[['Ability','Effort']].dot([.3,.7]) + np.random.randn(STUDENTS)*0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Taken altogether, this gives us a DataFrame of scores by student.\n This is more or less what the data I&rsquo;ll have at the end of the\n semester will look like (except that the names will be less silly).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Note that *lower* rankings are better, so flip sign on scores based on such rankings\nScores = pd.concat([students[['Final','Discussion']],\n                    -students[['Peers%d' % p for p in range(1,PROJECTS+1)]+['Project%d' % p for p in range(1,PROJECTS+1)]]],axis=1)\n\nprint(Scores.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluation\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "So, how do we turn a set of scores like this into course grades?\n There are two steps.  First, we compute the *singular value\n decomposition* (SVD) of the matrix of scores; this is a technique\n fundamental the the calculation of least squares regression\n techniques, and a popular tool in the recent machine learning\n literature.  It&rsquo;s closely related to an approach called &ldquo;principal\n components&rdquo; which has long been used in a field called\n &ldquo;psychometrics&rdquo;, which does things like designing and interpreting\n intelligence tests.\n\nA great feature of the SVD is that it allows us to *simultaneously*\nestimate ability+effort for each student, along with a weight for each\nassignment that indicates how informative that assignment is.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "from numpy.linalg import svd\n\n# Normalize scores (helps interpretation)\nScores = Scores - Scores.mean()\nScores = Scores/Scores.std()\n\ngrades,s,weights = svd(Scores,compute_uv=True)\nweights = pd.Series(weights[0,:],index=Scores.columns)\ngrades = pd.Series(grades[:,0],index=Scores.index)\n\n# Sign from SVD is indeterminate, but weights should be positive\nif weights.sum()<0:\n    weights = -weights\n    grades = - grades\n    \nweights = weights/weights.sum()\n\n# Normalize grades\ngrades = (grades-grades.mean())/grades.std()\n\nprint(weights)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The right measure of success for this approach is if the grades we assign provide a good estimate of the sum of ability and effort.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import cufflinks as cf\nfrom plotly.io import write_image\ncf.go_offline()\n\n# This is what we want to measure\ntruth = students[['Ability','Effort']].sum(axis=1) \n\ndf = pd.DataFrame({'Truth':truth,'Grades':grades})\nprint(df.corr().iloc[0,1])\n\ndf.iplot(kind='scatter', mode='markers', symbol='circle-dot',\n         x='Truth',y='Grades',\n         xTitle='Truth',yTitle='Grades',\n         asFigure=False)\n\n#write_image(fig,'grades_vs_truth.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Grade Assignment\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We&rsquo;ve described above how we&rsquo;ll calculate *scores*; how will these be\n turned into letter grades?  Let&rsquo;s start with a description of the\n distribution of scores from above; these have been /normalized, so\n that they have a mean of zero and a standard deviation of one, by\n construction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "grades.iplot(kind='histogram',\n             xTitle='Grade (raw)',\n             yTitle='Frequency')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To map into letter grades, we&rsquo;ll using the following device, which\n involves anchoring letter grades to the best five students.\n\n-   Let $\\bar{x}$ be the median grade among top five students (i.e.,\n    the 3rd highest grade, if we ignore ties).\n-   All students with a grade greater than $\\bar{x}-1/3$ will receive\n    an **A+** (so *at least* the best three students will receive this\n    grade, by construction).\n-   Remaining students within 2/3 of a standard deviation of $\\bar{x}$\n    will receive an **A**.\n-   Remaining students within one standard deviation of $\\bar{x}$\n    will receive an **A-**.\n-   And so on&#x2026;\n-   &#x2026;until students with grades more than 4 standard deviations of\n    $\\bar{x}$ will receive an **F**.\n\nIf scores are normally distributed, then we&rsquo;d expect $\\bar{x}$ to be\nabout 1.517 standard deviations above the mean, and for the\ndistribution of grades to be as follows.  (NB: The assumption of\nnormality is a big assumption! However, if it&rsquo;s satisfied then our\ndistribution of grades will be close to the distribution reported for\nall EEP classes: [http://projects.dailycal.org/grades/](http://projects.dailycal.org/grades/))  \n\n| Normalized Grade Score|Letter|Predicted %|\n|---|---|---|\n| $\\bar{x}-x\\leq 1/3$|A+|11.97%|\n| $2/3\\leq \\bar{x}-x < 1/3$|A|7.99%|\n| $1\\leq \\bar{x}-x< 2/3$|A-|10.55%|\n| $4/3\\leq \\bar{x}-x< 1$|B+|12.49%|\n| $5/3\\leq \\bar{x}-x< 4/3$|B|13.24%|\n| $2\\leq \\bar{x}-x< 5/3$|B-|12.57%|\n| $7/3\\leq \\bar{x}-x< 2$|C+|10.69%|\n| $8/3\\leq \\bar{x}-x< 7/3$|C|8.15%|\n| $3\\leq \\bar{x}-x< 8/3$|C-|5.56%|\n| $10/3\\leq \\bar{x}-x< 3$|D+|3.40%|\n| $11/3\\leq\\bar{x}- x< 10/3$|D|1.86%|\n| $4\\leq \\bar{x}-x< 11/3$|D-|0.91%|\n| $\\bar{x}-x > 4$|F|0.64%|\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Details\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "How do we know what the expected value of the third highest score is?\n For measures of centrality like the mean we have a nice theory\n governing its distribution (the mean will be asymptotically normally\n distributed with a standard deviation of $1/\\sqrt{N}$, where $N$ is\n the class size).  Similar results hold for estimating any *quantile*\n of the distribution, and follow from the Central Limit Theorem.  \n\nThese results *can&rsquo;t* be used for things like the value of the $k$th\nhighest score as the population gets large.  There is a collection of\ntheoretical results that *does* obtain, collectively called &ldquo;Extreme\nvalue theory&rdquo;.  \n\nInstead of heading to the math library, we will cheat.  Let&rsquo;s just\n**draw** a large number of samples of scores from a (quasi-) random\nnumber generator.  Each sample will just be the size of the class\n(e.g., 40).  Then for each sample we&rsquo;ll find the third-highest value,\nand compute the average across all the samples.  \n\nThis approach to calculating a statistic is called a &ldquo;Monte Carlo&rdquo;\nexperiment, and is often very effective (even if a bit crude).\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n\nSTUDENTS=40\n\nxbar=[]\nfor i in range(10000):\n    x = np.random.randn(STUDENTS)\n    x.sort()\n    xbar.append(x[-3]) \n\nprint(\"Estimated value of xbar is: {:2f}.\".format(np.mean(xbar)))\npd.DataFrame({\"xbar\":xbar}).iplot(kind='histogram',bins=100)"
      ]
    }
  ],
  "metadata": {
    "org": null,
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
